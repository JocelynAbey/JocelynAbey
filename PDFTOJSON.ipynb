{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0XhOcHh/5WXn1BNUpM8Ea",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JocelynAbey/JocelynAbey/blob/main/PDFTOJSON.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kss_PJ5RZIeA"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "# PDF processing libraries (with auto-install, useful in Colab)\n",
        "try:\n",
        "    import PyPDF2\n",
        "    import pdfplumber\n",
        "    import fitz  # PyMuPDF\n",
        "except ImportError:\n",
        "    import subprocess\n",
        "    import sys\n",
        "\n",
        "    packages = [\"PyPDF2\", \"pdfplumber\", \"PyMuPDF\"]\n",
        "    for package in packages:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "    import PyPDF2\n",
        "    import pdfplumber\n",
        "    import fitz\n",
        "\n",
        "# Logging setup\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "class PDFToDatasetConverter:\n",
        "    \"\"\"\n",
        "    Converts PDF documents containing Q&A pairs into chatbot training datasets.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Regex patterns for different Q&A formats\n",
        "        self.qa_patterns = [\n",
        "            # Pattern 1: Qnum: ... Anum: ...   (Q1: ... A1: ...)\n",
        "            r\"Q(\\d+):\\s*(.+?)\\s*A\\1:\\s*(.+?)(?=Q\\d+:|$)\",\n",
        "\n",
        "            # Pattern 2: Q: ... A: ...\n",
        "            r\"Q\\s*:\\s*(.+?)\\s*A\\s*:\\s*(.+?)(?=Q\\s*:|$)\",\n",
        "\n",
        "            # Pattern 3: Question: ... Answer: ...\n",
        "            r\"Question\\s*:\\s*(.+?)\\s*Answer\\s*:\\s*(.+?)(?=Question\\s*:|$)\",\n",
        "\n",
        "            # Pattern 4: Simple Q&A without prefixes (any sentence ending with ? and following text)\n",
        "            r\"([^.!?]*\\?)\\s*([^?]+?)(?=[^.!?]*\\?|$)\",\n",
        "        ]\n",
        "\n",
        "        # Category keywords for automatic categorization\n",
        "        self.category_keywords = {\n",
        "            \"departments\": [\n",
        "                \"department\", \"dept\", \"faculty\", \"school\", \"division\",\n",
        "                \"cse\", \"ece\", \"it\", \"civil\", \"ere\"\n",
        "            ],\n",
        "            \"fees\": [\n",
        "                \"fee\", \"cost\", \"tuition\", \"payment\", \"charge\", \"price\",\n",
        "                \"amount\", \"money\", \"rupees\", \"₹\"\n",
        "            ],\n",
        "            \"facilities\": [\n",
        "                \"library\", \"canteen\", \"hostel\", \"parking\", \"gym\", \"lab\",\n",
        "                \"toilet\", \"restroom\", \"wifi\"\n",
        "            ],\n",
        "            \"contact\": [\n",
        "                \"contact\", \"phone\", \"email\", \"address\", \"office\",\n",
        "                \"reception\", \"call\", \"reach\"\n",
        "            ],\n",
        "            \"admissions\": [\n",
        "                \"admission\", \"application\", \"entrance\", \"exam\", \"keam\",\n",
        "                \"apply\", \"eligibility\"\n",
        "            ],\n",
        "            \"programs\": [\n",
        "                \"program\", \"course\", \"degree\", \"btech\", \"mtech\", \"diploma\",\n",
        "                \"engineering\"\n",
        "            ],\n",
        "            \"location\": [\n",
        "                \"where\", \"located\", \"address\", \"place\", \"building\",\n",
        "                \"floor\", \"room\"\n",
        "            ],\n",
        "            \"timing\": [\n",
        "                \"time\", \"hours\", \"schedule\", \"timing\", \"open\", \"close\", \"when\"\n",
        "            ],\n",
        "            \"leadership\": [\n",
        "                \"principal\", \"hod\", \"head\", \"director\", \"dean\",\n",
        "                \"faculty\", \"professor\"\n",
        "            ],\n",
        "            \"placements\": [\n",
        "                \"placement\", \"job\", \"career\", \"company\", \"recruiter\",\n",
        "                \"employment\"\n",
        "            ],\n",
        "            \"activities\": [\n",
        "                \"club\", \"association\", \"society\", \"event\", \"activity\", \"sports\"\n",
        "            ],\n",
        "        }\n",
        "\n",
        "    # ---------------- PDF TEXT EXTRACTION ---------------- #\n",
        "\n",
        "    def extract_text_pypdf2(self, pdf_path: str) -> str:\n",
        "        \"\"\"Extract text using PyPDF2.\"\"\"\n",
        "        try:\n",
        "            with open(pdf_path, \"rb\") as file:\n",
        "                reader = PyPDF2.PdfReader(file)\n",
        "                text = \"\"\n",
        "                for page in reader.pages:\n",
        "                    page_text = page.extract_text()\n",
        "                    if page_text:\n",
        "                        text += page_text + \"\\n\"\n",
        "                return text\n",
        "        except Exception as e:\n",
        "            logger.error(f\"PyPDF2 extraction failed: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def extract_text_pdfplumber(self, pdf_path: str) -> str:\n",
        "        \"\"\"Extract text using pdfplumber (better for complex layouts).\"\"\"\n",
        "        try:\n",
        "            text = \"\"\n",
        "            with pdfplumber.open(pdf_path) as pdf:\n",
        "                for page in pdf.pages:\n",
        "                    page_text = page.extract_text()\n",
        "                    if page_text:\n",
        "                        text += page_text + \"\\n\"\n",
        "            return text\n",
        "        except Exception as e:\n",
        "            logger.error(f\"pdfplumber extraction failed: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def extract_text_pymupdf(self, pdf_path: str) -> str:\n",
        "        \"\"\"Extract text using PyMuPDF (fast and accurate).\"\"\"\n",
        "        try:\n",
        "            doc = fitz.open(pdf_path)\n",
        "            text = \"\"\n",
        "            for page in doc:\n",
        "                text += page.get_text() + \"\\n\"\n",
        "            doc.close()\n",
        "            return text\n",
        "        except Exception as e:\n",
        "            logger.error(f\"PyMuPDF extraction failed: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def extract_text_from_pdf(self, pdf_path: str, method: str = \"auto\") -> str:\n",
        "        \"\"\"\n",
        "        Extract text from PDF using specified method.\n",
        "\n",
        "        method: 'auto', 'pymupdf', 'pdfplumber', or 'pypdf2'\n",
        "        \"\"\"\n",
        "        logger.info(f\"Extracting text from {pdf_path} using method: {method}\")\n",
        "\n",
        "        if method == \"auto\":\n",
        "            methods = [\n",
        "                (\"pymupdf\", self.extract_text_pymupdf),\n",
        "                (\"pdfplumber\", self.extract_text_pdfplumber),\n",
        "                (\"pypdf2\", self.extract_text_pypdf2),\n",
        "            ]\n",
        "            for method_name, func in methods:\n",
        "                text = func(pdf_path)\n",
        "                if text and text.strip():\n",
        "                    logger.info(f\"Successfully extracted text using {method_name}\")\n",
        "                    return text\n",
        "            logger.error(\"All extraction methods failed\")\n",
        "            return \"\"\n",
        "\n",
        "        elif method == \"pymupdf\":\n",
        "            return self.extract_text_pymupdf(pdf_path)\n",
        "        elif method == \"pdfplumber\":\n",
        "            return self.extract_text_pdfplumber(pdf_path)\n",
        "        elif method == \"pypdf2\":\n",
        "            return self.extract_text_pypdf2(pdf_path)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown extraction method: {method}\")\n",
        "\n",
        "    # ---------------- TEXT CLEANING & Q&A EXTRACTION ---------------- #\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"Clean and normalize extracted text.\"\"\"\n",
        "        # Collapse multiple spaces/newlines into single space\n",
        "        text = re.sub(r\"\\s+\", \" \", text)\n",
        "\n",
        "        # (Optional) further cleaning can be added here if needed\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def clean_qa_text(self, text: str) -> str:\n",
        "        \"\"\"Clean individual question or answer text.\"\"\"\n",
        "        text = text.strip()\n",
        "\n",
        "        prefixes = [\"Q:\", \"A:\", \"Question:\", \"Answer:\", \"Ans:\", \"•\", \"-\", \"*\"]\n",
        "        for prefix in prefixes:\n",
        "            if text.startswith(prefix):\n",
        "                text = text[len(prefix):].strip()\n",
        "\n",
        "        # Remove trailing colons\n",
        "        text = text.rstrip(\":\").strip()\n",
        "\n",
        "        # If there's a '?' inside but not at the end, move it to the end\n",
        "        if \"?\" in text and not text.endswith(\"?\"):\n",
        "            text = text.replace(\"?\", \"\")\n",
        "            text = text.strip() + \"?\"\n",
        "\n",
        "        return text\n",
        "\n",
        "    def is_valid_qa_pair(self, question: str, answer: str) -> bool:\n",
        "        \"\"\"Validate if a Q&A pair is meaningful.\"\"\"\n",
        "        # Minimum length\n",
        "        if len(question) < 5 or len(answer) < 5:\n",
        "            return False\n",
        "\n",
        "        # Maximum length (avoid gigantic chunks)\n",
        "        if len(question) > 500 or len(answer) > 2000:\n",
        "            return False\n",
        "\n",
        "        # Question should look like a question\n",
        "        question_indicators = [\n",
        "            \"what\", \"where\", \"when\", \"how\", \"who\", \"why\", \"which\",\n",
        "            \"can\", \"is\", \"are\", \"do\", \"does\", \"?\", \"will\", \"shall\"\n",
        "        ]\n",
        "        if not any(ind in question.lower() for ind in question_indicators):\n",
        "            return False\n",
        "\n",
        "        # Answer should be at least 3 words\n",
        "        if len(answer.split()) < 3:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def extract_qa_pairs(self, text: str) -> List[Tuple[str, str]]:\n",
        "        \"\"\"\n",
        "        Extract Q&A pairs from text using multiple patterns.\n",
        "        Returns a list of (question, answer) tuples.\n",
        "        \"\"\"\n",
        "        qa_pairs: List[Tuple[str, str]] = []\n",
        "\n",
        "        for pattern in self.qa_patterns:\n",
        "            matches = re.findall(pattern, text, flags=re.IGNORECASE | re.DOTALL)\n",
        "            for match in matches:\n",
        "                # Handle numbered Q1/A1 pattern\n",
        "                if len(match) == 3:\n",
        "                    question, answer = match[1], match[2]\n",
        "                elif len(match) == 2:\n",
        "                    question, answer = match\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "                question = self.clean_qa_text(question)\n",
        "                answer = self.clean_qa_text(answer)\n",
        "\n",
        "                if self.is_valid_qa_pair(question, answer):\n",
        "                    qa_pairs.append((question, answer))\n",
        "\n",
        "        # Remove duplicates while preserving order\n",
        "        seen = set()\n",
        "        unique_pairs: List[Tuple[str, str]] = []\n",
        "        for q, a in qa_pairs:\n",
        "            key = (q.lower().strip(), a.lower().strip())\n",
        "            if key not in seen:\n",
        "                seen.add(key)\n",
        "                unique_pairs.append((q, a))\n",
        "\n",
        "        logger.info(f\"Extracted {len(unique_pairs)} unique Q&A pairs\")\n",
        "        return unique_pairs\n",
        "\n",
        "    # ---------------- CATEGORIZATION & DATASET CREATION ---------------- #\n",
        "\n",
        "    def categorize_qa_pair(self, question: str, answer: str) -> str:\n",
        "        \"\"\"Automatically categorize Q&A pair based on keywords.\"\"\"\n",
        "        text = (question + \" \" + answer).lower()\n",
        "\n",
        "        category_scores: Dict[str, int] = {}\n",
        "        for category, keywords in self.category_keywords.items():\n",
        "            score = sum(1 for kw in keywords if kw in text)\n",
        "            if score > 0:\n",
        "                category_scores[category] = score\n",
        "\n",
        "        if category_scores:\n",
        "            # Return category with highest score\n",
        "            return max(category_scores, key=category_scores.get)\n",
        "        else:\n",
        "            return \"general\"\n",
        "\n",
        "    def create_dataset(self, qa_pairs: List[Tuple[str, str]]) -> List[Dict]:\n",
        "        \"\"\"Convert Q&A pairs to chatbot dataset format.\"\"\"\n",
        "        dataset: List[Dict] = []\n",
        "        for question, answer in qa_pairs:\n",
        "            category = self.categorize_qa_pair(question, answer)\n",
        "            dataset.append(\n",
        "                {\n",
        "                    \"question\": question,\n",
        "                    \"answer\": answer,\n",
        "                    \"category\": category,\n",
        "                }\n",
        "            )\n",
        "        return dataset\n",
        "\n",
        "    # ---------------- DATASET ENHANCEMENT ---------------- #\n",
        "\n",
        "    def generate_question_variations(self, question: str) -> List[str]:\n",
        "        \"\"\"Generate simple variations of a question.\"\"\"\n",
        "        variations = [question]\n",
        "\n",
        "        transformations = [\n",
        "            # Remove some initial question forms\n",
        "            (r\"^What is \", \"\"),\n",
        "            (r\"^Where is \", \"\"),\n",
        "            (r\"^How can I \", \"\"),\n",
        "            (r\"\\?$\", \"\"),\n",
        "            # Add prefixes\n",
        "            (\"\", \"Tell me about \"),\n",
        "            (\"\", \"Information about \"),\n",
        "            (\"\", \"Details about \"),\n",
        "        ]\n",
        "\n",
        "        for old_pattern, new_pattern in transformations:\n",
        "            if old_pattern:\n",
        "                new_q = re.sub(old_pattern, new_pattern, question, flags=re.IGNORECASE)\n",
        "            else:\n",
        "                new_q = new_pattern + question\n",
        "\n",
        "            if new_q != question and new_q not in variations:\n",
        "                variations.append(new_q)\n",
        "\n",
        "        return variations\n",
        "\n",
        "    def enhance_dataset(self, dataset: List[Dict]) -> List[Dict]:\n",
        "        \"\"\"Enhance dataset with question variations.\"\"\"\n",
        "        enhanced = list(dataset)  # copy\n",
        "        variations_list = []\n",
        "\n",
        "        for item in dataset:\n",
        "            q = item[\"question\"]\n",
        "            a = item[\"answer\"]\n",
        "            c = item[\"category\"]\n",
        "\n",
        "            q_variations = self.generate_question_variations(q)\n",
        "            for v in q_variations:\n",
        "                if v != q:\n",
        "                    variations_list.append(\n",
        "                        {\n",
        "                            \"question\": v,\n",
        "                            \"answer\": a,\n",
        "                            \"category\": c,\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "        enhanced.extend(variations_list)\n",
        "        logger.info(f\"Enhanced dataset from {len(dataset)} to {len(enhanced)} entries\")\n",
        "        return enhanced\n",
        "\n",
        "    # ---------------- SAVE & STATS ---------------- #\n",
        "\n",
        "    def save_dataset(self, dataset: List[Dict], output_path: str, format: str = \"json\"):\n",
        "        \"\"\"Save dataset to JSON or CSV.\"\"\"\n",
        "        format = format.lower()\n",
        "        if format == \"json\":\n",
        "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(dataset, f, indent=2, ensure_ascii=False)\n",
        "        elif format == \"csv\":\n",
        "            df = pd.DataFrame(dataset)\n",
        "            df.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported format: {format}\")\n",
        "\n",
        "        logger.info(f\"Dataset saved to {output_path} ({len(dataset)} entries)\")\n",
        "\n",
        "    def generate_statistics(self, dataset: List[Dict]) -> Dict:\n",
        "        \"\"\"Generate simple statistics about the dataset.\"\"\"\n",
        "        if not dataset:\n",
        "            return {}\n",
        "\n",
        "        categories = [item[\"category\"] for item in dataset]\n",
        "        category_counts: Dict[str, int] = {}\n",
        "        for c in categories:\n",
        "            category_counts[c] = category_counts.get(c, 0) + 1\n",
        "\n",
        "        avg_q_len = sum(len(item[\"question\"]) for item in dataset) / len(dataset)\n",
        "        avg_a_len = sum(len(item[\"answer\"]) for item in dataset) / len(dataset)\n",
        "\n",
        "        stats = {\n",
        "            \"total_pairs\": len(dataset),\n",
        "            \"categories\": category_counts,\n",
        "            \"avg_question_length\": round(avg_q_len, 2),\n",
        "            \"avg_answer_length\": round(avg_a_len, 2),\n",
        "        }\n",
        "        return stats\n",
        "\n",
        "    # ---------------- MAIN CONVERSION API ---------------- #\n",
        "\n",
        "    def convert_pdf_to_dataset(\n",
        "        self,\n",
        "        pdf_path: str,\n",
        "        output_path: str | None = None,\n",
        "        format: str = \"json\",\n",
        "        extraction_method: str = \"auto\",\n",
        "        enhance: bool = True,\n",
        "    ) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        High-level function: PDF -> dataset (list of dicts).\n",
        "        Optionally saves to JSON/CSV if output_path is given.\n",
        "        \"\"\"\n",
        "        logger.info(\"Starting PDF to dataset conversion\")\n",
        "\n",
        "        # Extract text\n",
        "        text = self.extract_text_from_pdf(pdf_path, method=extraction_method)\n",
        "        if not text:\n",
        "            raise ValueError(\"Failed to extract text from PDF\")\n",
        "\n",
        "        # Clean text\n",
        "        cleaned_text = self.clean_text(text)\n",
        "\n",
        "        # Extract Q&A pairs\n",
        "        qa_pairs = self.extract_qa_pairs(cleaned_text)\n",
        "        if not qa_pairs:\n",
        "            raise ValueError(\"No Q&A pairs found in PDF\")\n",
        "\n",
        "        # Convert to dataset\n",
        "        dataset = self.create_dataset(qa_pairs)\n",
        "\n",
        "        # Enhance (optional)\n",
        "        if enhance:\n",
        "            dataset = self.enhance_dataset(dataset)\n",
        "\n",
        "        # Save if path provided\n",
        "        if output_path:\n",
        "            # If output_path has no extension, infer from format\n",
        "            output_path = str(output_path)\n",
        "            if \".\" not in Path(output_path).name:\n",
        "                output_path = output_path + f\".{format}\"\n",
        "            self.save_dataset(dataset, output_path, format=format)\n",
        "\n",
        "        # Stats\n",
        "        stats = self.generate_statistics(dataset)\n",
        "        logger.info(f\"Dataset statistics: {stats}\")\n",
        "\n",
        "        return dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()  # choose dataset.pdf from your computer\n",
        "print(uploaded.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "vyTdGBybZeaY",
        "outputId": "82b9387c-57d1-453c-e843-19d309ee28fb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-de8504cf-75f9-457a-8b0c-bea8bf12d098\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-de8504cf-75f9-457a-8b0c-bea8bf12d098\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving dataset2.pdf to dataset2.pdf\n",
            "dict_keys(['dataset2.pdf'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "converter = PDFToDatasetConverter()\n",
        "\n",
        "pdf_path = \"dataset2.pdf\"                  # or the exact filename you uploaded\n",
        "output_path = \"chatbot_dataset.json\"              # JSON will be saved in /content/\n",
        "\n",
        "dataset = converter.convert_pdf_to_dataset(\n",
        "    pdf_path=pdf_path,\n",
        "    output_path=output_path,\n",
        "    format=\"json\",         # or \"csv\" if you want CSV instead\n",
        "    extraction_method=\"auto\",\n",
        "    enhance=True           # set False if you don't want extra question variations\n",
        ")\n",
        "\n",
        "print(\"Done! JSON saved as:\", output_path)\n",
        "print(\"Total Q&A entries in dataset:\", len(dataset))\n",
        "dataset[:3]   # show first 3 entries\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4eh49EraelS",
        "outputId": "e7634a97-2ad4-46d1-abbf-267959914e9a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done! JSON saved as: chatbot_dataset.json\n",
            "Total Q&A entries in dataset: 1199\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'question': 'What is LBS Institute of Technology for Women (LBSITW), Poojappura?',\n",
              "  'answer': \"LBS Institute of Technology for Women (LBSITW), Poojappura is a government women's engineering college located in Poojappura, Thiruvananthapuram, Kerala, India. Established in 2001, it is affiliated to APJ Abdul Kalam Technological University (KTU) and is the first government engineering college exclusively for women in Kerala, offering undergraduate, postgraduate, and doctoral programs in engineering disciplines.\",\n",
              "  'category': 'programs'},\n",
              " {'question': 'When was LBS Institute of Technology for Women (LBSITW), Poojappura established?',\n",
              "  'answer': 'LBS Institute of Technology for Women (LBSITW), Poojappura was established in 2001 as a government cost-sharing institution under the LBS Centre for Science and Technology.',\n",
              "  'category': 'departments'},\n",
              " {'question': 'What makes LBS Institute of Technology for Women (LBSITW), Poojappura unique in Kerala?',\n",
              "  'answer': \"LBSITW, Poojappura is unique as the first and only government women's engineering college in Kerala, ranked 1st in KTU B.Tech pass percentage for the 2021-2025 batch. It emphasizes women's empowerment in STEM, with NAAC accreditation and a focus on research and innovation through Ph.D. programs.\",\n",
              "  'category': 'programs'}]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"chatbot_dataset.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "7_FMJLB4anHY",
        "outputId": "21040707-97ce-42f0-f106-46fb6d7827c6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e794c461-5385-4690-b4e9-06a8fee9b78a\", \"chatbot_dataset.json\", 330565)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}